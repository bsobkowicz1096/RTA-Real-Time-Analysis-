# ğŸ“Š Zadanie 2 - Strumieniowanie danych w Apache Spark

## ğŸ“ Opis
Implementacja zadaÅ„ z laboratorium 7 dotyczÄ…cych strumieniowania danych, agregacji i segmentacji klientÃ³w w czasie rzeczywistym przy uÅ¼yciu Apache Spark Structured Streaming.

## ğŸ“¥ Pobranie repozytorium

Aby pobraÄ‡ to repozytorium, wykonaj nastÄ™pujÄ…ce kroki:

```bash
# Klonuj repozytorium
git clone https://github.com/bsobkowicz1096/Real_Time_Analysis.git

# PrzejdÅº do katalogu projektu
cd Real_Time_Analysis/Zadanie\ 2
```

## ğŸ“‚ Struktura projektu
- `generator.py` - Generator danych JSON do Ä‡wiczeÅ„
- `rate_source.py` - Wykorzystanie rate jako ÅºrÃ³dÅ‚a kontrolowanego strumienia
- `filtering.py` - Filtrowanie danych bez agregacji (append mode)
- `json_source.py` - Odczyt strumieniowy z plikÃ³w JSON
- `counting.py` - Bezstanowe zliczanie zdarzeÅ„
- `time_windows.py` - Agregacja w tumbling windows (staÅ‚e okna czasowe)
- `sliding_window.py` - Agregacja w sliding windows (przesuwne okna czasowe)
- `segmentation.py` - Segmentacja klientÃ³w w czasie rzeczywistym
- `requirements.txt` - Lista wymaganych bibliotek

## âš™ï¸ FunkcjonalnoÅ›ci
- Generowanie kontrolowanych strumieni danych
- Przetwarzanie danych JSON w czasie rzeczywistym
- Filtrowanie i agregacja danych strumieniowych
- Implementacja okien czasowych (tumbling, sliding)
- Segmentacja klientÃ³w na podstawie ich zachowaÅ„
- RÃ³Å¼ne tryby wyjÅ›ciowe: append, update, complete

## ğŸ› ï¸ Wymagania
- ğŸ Python 3.x
- âš¡ Apache Spark
- ğŸ“¦ PySpark
- ğŸ”„ Kafka-Python

Wszystkie wymagane biblioteki Pythona moÅ¼na zainstalowaÄ‡ za pomocÄ…:
```bash
pip install -r requirements.txt
```

## ğŸš€ Instrukcja uruchomienia

### 1. Instalacja zaleÅ¼noÅ›ci:
```bash
# Instalacja wymaganych bibliotek
pip install -r requirements.txt
```

### 2. Generator danych (dla zadaÅ„ z JSON):
```bash
# UtwÃ³rz katalog dla danych strumieniowych
mkdir -p data/stream

# Uruchom generator w tle
python generator.py &
```

### 3. Uruchomienie zadaÅ„:
```bash
# Uruchom wybrane zadanie
spark-submit rate_source.py
```

## ğŸ“Š Opis zadaÅ„

### 1. Rate Source ğŸ“ˆ
Generuje strumieÅ„ danych z okreÅ›lonÄ… prÄ™dkoÅ›ciÄ… (5 wierszy/sekundÄ™) i dodaje kolumny identyfikujÄ…ce uÅ¼ytkownika i typ zdarzenia.

### 2. Filtering ğŸ”
Filtruje dane strumieniowe, wyÅ›wietlajÄ…c tylko zdarzenia typu "purchase".

### 3. JSON Source ğŸ“„
Odczytuje dane JSON generowane w czasie rzeczywistym przez generator.py.

### 4. Counting ğŸ”¢
Zlicza zdarzenia pogrupowane wedÅ‚ug typu.

### 5. Time Windows â°
Grupuje zdarzenia w 5-minutowych oknach czasowych i zlicza je dla kaÅ¼dego typu zdarzenia.

### 6. Sliding Window ğŸ”„
Grupuje zdarzenia w przesuwajÄ…cych siÄ™ 5-minutowych oknach czasowych z 1-minutowym przesuniÄ™ciem.

### 7. Segmentation ğŸ‘¥
Segmentuje uÅ¼ytkownikÃ³w na kategorie "Buyer", "Cart abandoner" i "Lurker" w oparciu o ich zachowanie.

## ğŸ’¡ Uwagi
- Generator tworzy pliki JSON w katalogu `data/stream` co 5 sekund
- Wszystkie programy automatycznie zatrzymujÄ… siÄ™ po przetworzeniu 5 partii danych
- Dane sÄ… przetwarzane strumieniowo w czasie rzeczywistym
- Watermark pozwala na zarzÄ…dzanie opÃ³Åºnionymi danymi i zwalnianie pamiÄ™ci
- BÅ‚Ä…d "SparkContext has been shutdown" po 5 partiach jest oczekiwanym zachowaniem, poniewaÅ¼ aplikacja celowo zatrzymuje Spark po tej liczbie przetworzonych partii

## ğŸ”§ Technologie
- ğŸ Python
- âš¡ Apache Spark
- ğŸ“Š Structured Streaming
- ğŸ“¦ PySpark
